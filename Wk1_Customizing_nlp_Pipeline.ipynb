{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d77a2df-7be8-450c-a1dc-5e009a33c79b",
   "metadata": {},
   "source": [
    "# Customizing NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e850a-48f1-41f4-8cf3-bbb12b367741",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "- Customize SpaCy's nlp pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1298b-4a9c-4267-999d-06a19a396072",
   "metadata": {},
   "source": [
    "### Customizing the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b297f-a6b6-4694-b89e-c44feee8f471",
   "metadata": {},
   "source": [
    "We learned how to customize stop words when making word clouds, but we can also customize a SpaCy nlp pipeline. In this lesson we will demonstrate how to:\n",
    "- Add stopwords\n",
    "- Remove stopwords\n",
    "- Tokenize contractions\n",
    "\n",
    "Once we demonstrate each technique, we will define a custom function to create the pipeline.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1340958e-812b-48bb-81aa-bf454986c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ee9048-c285-491b-b941-3efd611a31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions\n",
    "def batch_preprocess_texts(\n",
    "    texts,\n",
    "    nlp=None,\n",
    "    remove_stopwords=True,\n",
    "    remove_punct=True,\n",
    "    use_lemmas=False,\n",
    "    disable=[\"ner\"],\n",
    "    batch_size=50,\n",
    "    n_process=-1,\n",
    "):\n",
    "    \"\"\"Efficiently preprocess a collection of texts using nlp.pipe()\n",
    "\n",
    "    Args:\n",
    "        texts (collection of strings): collection of texts to process (e.g. df['text'])\n",
    "        nlp (spacy pipe), optional): Spacy nlp pipe. Defaults to None; if None, it creates a default 'en_core_web_sm' pipe.\n",
    "        remove_stopwords (bool, optional): Controls stopword removal. Defaults to True.\n",
    "        remove_punct (bool, optional): Controls punctuation removal. Defaults to True.\n",
    "        use_lemmas (bool, optional): lemmatize tokens. Defaults to False.\n",
    "        disable (list of strings, optional): named pipeline elements to disable. Defaults to [\"ner\"]: Used with nlp.pipe(disable=disable)\n",
    "        batch_size (int, optional): Number of texts to process in a batch. Defaults to 50.\n",
    "        n_process (int, optional): Number of CPU processors to use. Defaults to -1 (meaning all CPU cores).\n",
    "\n",
    "    Returns:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "    # from tqdm.notebook import tqdm\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    processed_texts = []\n",
    "\n",
    "    for doc in tqdm(nlp.pipe(texts, disable=disable, batch_size=batch_size, n_process=n_process)):\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_stopwords == True) and (token.is_stop == True):\n",
    "                # Continue the loop with the next token\n",
    "                continue\n",
    "\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_punct == True):\n",
    "                continue\n",
    "\n",
    "            # Check if should remove stopwords and if token is stopword\n",
    "            if (remove_punct == True) and (token.is_space == True):\n",
    "                continue\n",
    "\n",
    "            \n",
    "            ## Determine final form of output list of tokens/lemmas\n",
    "            if use_lemmas:\n",
    "                tokens.append(token.lemma_.lower())\n",
    "            else:\n",
    "                tokens.append(token.text.lower())\n",
    "\n",
    "        processed_texts.append(tokens)\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc8a1c-050e-4631-b1ab-d9492f8ebc13",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206e98d3-666c-4339-b434-f10055ad4485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"While running in Central Park, I noticed that the constant buzzing of flies was annoying. I don't like flies, but I couldn't be too upset as they were likely attracted to the McDonald's food that someone carelessly dropped. I wondered, 'How can they be so uncaring?'\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "# Define sample text\n",
    "sample_text = \"While running in Central Park, I noticed that the constant buzzing of flies was annoying. I don't like flies, but I couldn't be too upset as they were likely attracted to the McDonald's food that someone carelessly dropped. I wondered, 'How can they be so uncaring?'\"\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec366728-8d4a-48d9-9289-c7acea2ee84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:21, 21.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['while', 'running', 'in', 'central', 'park', 'i', 'noticed', 'that', 'the', 'constant', 'buzzing', 'of', 'flies', 'was', 'annoying', 'i', 'do', \"n't\", 'like', 'flies', 'but', 'i', 'could', \"n't\", 'be', 'too', 'upset', 'as', 'they', 'were', 'likely', 'attracted', 'to', 'the', 'mcdonald', \"'s\", 'food', 'that', 'someone', 'carelessly', 'dropped', 'i', 'wondered', 'how', 'can', 'they', 'be', 'so', 'uncaring']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Default nlp pipeline and keep all stopwords\n",
    "tokens_keep_all_stop = batch_preprocess_texts([sample_text], nlp = nlp_light, remove_stopwords = False)\n",
    "print(tokens_keep_all_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a48c302-8ea4-49b7-a0ce-de4d559c1246",
   "metadata": {},
   "source": [
    "Now run the default pipeline and remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f70eeeb-1757-478f-b36c-7c43932edeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:21, 21.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['running', 'central', 'park', 'noticed', 'constant', 'buzzing', 'flies', 'annoying', 'like', 'flies', 'upset', 'likely', 'attracted', 'mcdonald', 'food', 'carelessly', 'dropped', 'wondered', 'uncaring']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove default stopwords\n",
    "tokens_remove_default_stop = batch_preprocess_texts([sample_text])\n",
    "print(tokens_remove_default_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8434e56-a464-40ec-b995-579a6b1660b8",
   "metadata": {},
   "source": [
    "The next code is a simple loop to compare the two lists of tokens we just created. The result will be that we can easily see which words were removed by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c7c147-2f68-477d-9a2e-54a46646fbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['while',\n",
       "  'running',\n",
       "  'in',\n",
       "  'central',\n",
       "  'park',\n",
       "  'i',\n",
       "  'noticed',\n",
       "  'that',\n",
       "  'the',\n",
       "  'constant',\n",
       "  'buzzing',\n",
       "  'of',\n",
       "  'flies',\n",
       "  'was',\n",
       "  'annoying',\n",
       "  'i',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'like',\n",
       "  'flies',\n",
       "  'but',\n",
       "  'i',\n",
       "  'could',\n",
       "  \"n't\",\n",
       "  'be',\n",
       "  'too',\n",
       "  'upset',\n",
       "  'as',\n",
       "  'they',\n",
       "  'were',\n",
       "  'likely',\n",
       "  'attracted',\n",
       "  'to',\n",
       "  'the',\n",
       "  'mcdonald',\n",
       "  \"'s\",\n",
       "  'food',\n",
       "  'that',\n",
       "  'someone',\n",
       "  'carelessly',\n",
       "  'dropped',\n",
       "  'i',\n",
       "  'wondered',\n",
       "  'how',\n",
       "  'can',\n",
       "  'they',\n",
       "  'be',\n",
       "  'so',\n",
       "  'uncaring']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looop to find words removed\n",
    "removed_tokens = []\n",
    "for token in tokens_keep_all_stop:\n",
    "    if token not in tokens_remove_default_stop:\n",
    "        removed_tokens.append(token)\n",
    "removed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac926a26-ab7b-4dc0-85ad-b16596448f6e",
   "metadata": {},
   "source": [
    "The list above includes all of the words that were removed from our simple sample text.\n",
    "\n",
    "We can customize the stopwords. First we can obtain the entire set of default stopwords to use as the starting point for our custom list.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf1538f3-f104-4ceb-a738-57dc8ce7f310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define custom nlp pipeline\n",
    "custom_nlp = spacy.load('en_core_web_sm')\n",
    "# Let's start by accessing spaCy's default stopwords\n",
    "spacy_stopwords = custom_nlp.Defaults.stop_words\n",
    "spacy_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0948e2fe-f775-43f0-935a-0e0e77e76cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many default stopwords?\n",
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a430c-66f8-46b3-93e9-1600777958f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Adding Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbeff6e-54e0-46c5-baa5-a0d7da801393",
   "metadata": {},
   "source": [
    "For this demo we want to add the words \"food\", \"likely\", \"upset\", and \"carelessly\" to the list of stopwords. Remember that adding a word to the stopwords list will cause it to be removed from the final list of tokens when stopwords are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d1e974e-bfba-4c94-b87a-361ce495b179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can include additional stopwords by adding them to the default set\n",
    "# Add custom stopwords\n",
    "custom_stopwords = [\"food\", \"likely\",'upset','carelessly']\n",
    "for word in custom_stopwords:\n",
    "    # Add the word to the list of stopwords (for easily tracking stopwords)\n",
    "    custom_nlp.Defaults.stop_words.add(word)\n",
    "    # Set the is_stop attribute for the word in the vocab dict to true. \n",
    "    # this is what will actually determine spacy treating the word as a stop word\n",
    "    custom_nlp.vocab[word].is_stop = True\n",
    "updated_spacy_stopwords = custom_nlp.Defaults.stop_words\n",
    "len(updated_spacy_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da138773-21dd-4cda-8964-7bb61b893161",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6409e-4b95-49d7-881b-3c4e59926ab5",
   "metadata": {},
   "source": [
    "We can remove stopwords by discarding them. We will remove \"but\" and \"someone\" from our custom list of stopwords. Remember that removing a word from the stopwords means it will be included in the final list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "694bd105-9df0-4770-9aac-c26af96070c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "remove_stopwords = [\"but\", \"someone\"]\n",
    "for word in remove_stopwords:\n",
    "    custom_nlp.Defaults.stop_words.discard(word)\n",
    "    # Ensure the words are not recognized as stopwords\n",
    "    custom_nlp.vocab[word].is_stop = False\n",
    "updated_spacy_stopwords = custom_nlp.Defaults.stop_words\n",
    "len(updated_spacy_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84399ea9-bb31-4828-b7ac-dfcd45f26285",
   "metadata": {},
   "source": [
    "Now that we have defined the list of stopwords we wish to apply, we can call our custom function with our customized nlp pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebfbe751-f9f7-4692-a5e1-b10f394fee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:21, 21.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['running', 'central', 'park', 'noticed', 'constant', 'buzzing', 'flies', 'annoying', 'like', 'flies', 'but', 'attracted', 'mcdonald', 'someone', 'dropped', 'wondered', 'uncaring']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process text with custom nlp pipeline\n",
    "custom_stopwords_removed = batch_preprocess_texts([sample_text], nlp = custom_nlp)\n",
    "print(custom_stopwords_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80099ce1-056e-434b-8ac4-c2d9592b82d5",
   "metadata": {},
   "source": [
    "We see that we have eliminated \"food\", \"likely\", \"upset\", and \"carelessly\" from our final list of tokens. We have also included \"but\" and \"someone.\" Remember, that this is just a demonstration of the techniques; the choice of which words to add or remove is dependent on your specific text and goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64d1a7-82c0-4917-8b1e-4103e23e55ac",
   "metadata": {},
   "source": [
    "**Contractions**\n",
    "\n",
    "You may have noticed that the original text contained two contractions: \"don't\" and \"couldn't\". When tokenized, \"don't\" was split into \"do\" and \"n't\". \"Couldn't\" was split into \"could\" and \"n't.\" Also, note that \"do\", \"could\" and \"n't\" were all on the default stopwords list. Depending on your problem, you may want to keep contractions whole.\n",
    "\n",
    "Below we will demonstrate how to keep contrations intact by using .add_special_case to the nlp pipeline tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "408b78b9-20c6-4d81-a71c-6ce71f1fc181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:21, 21.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['running', 'central', 'park', 'noticed', 'constant', 'buzzing', 'flies', 'annoying', \"don't\", 'like', 'flies', 'but', \"couldn't\", 'attracted', 'mcdonald', 'someone', 'dropped', 'wondered', 'uncaring']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of contractions to keep as single tokens\n",
    "contractions = [\"don't\", \"couldn't\"]\n",
    "# Loop through the contractions list and add special cases\n",
    "for contraction in contractions:\n",
    "    special_case = [{\"ORTH\": contraction}]\n",
    "    custom_nlp.tokenizer.add_special_case(contraction, special_case)\n",
    "keep_contractions = batch_preprocess_texts([sample_text], nlp = custom_nlp)\n",
    "print(keep_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece75bd-5bb6-4321-88d4-2d33a6985a43",
   "metadata": {},
   "source": [
    "Now, we have \"don't and \"couldn't\" included in our final list of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd90161-5a08-43e7-9d70-d28ccc2be1b4",
   "metadata": {},
   "source": [
    "### Custom nlp pipeline function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21ac81-5261-414e-ac5f-99ebf8ca7480",
   "metadata": {},
   "source": [
    "To allow us to customize the pipeline efficiently, we will define a custom function, including the code we demonstrated above. We will add two additional arguments. \"Disable\" will allow us to disable components of the pipeline when calling the function. We will also allow for the customization of which spacy language model to use. We will learn more about this in a future lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ec605a0-c6ce-4e00-a250-cf339d978f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_custom_nlp(\n",
    "    disable=[\"ner\"],\n",
    "    contractions=[\"don't\", \"can't\", \"couldn't\", \"you'd\", \"I'll\"],\n",
    "    stopwords_to_add=[],\n",
    "    stopwords_to_remove=[],\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    "):\n",
    "    \"\"\"Returns a custom spacy nlp pipeline.\n",
    "    \n",
    "    Args:\n",
    "        disable (list, optional): Names of pipe components to disable. Defaults to [\"ner\"].\n",
    "        contractions (list, optional): List of contractions to add as special cases. Defaults to [\"don't\", \"can't\", \"couldn't\", \"you'd\", \"I'll\"].\n",
    "        stopwords_to_add(list, optional): List of words to set as stopwords (word.is_stop=True)\n",
    "        stopwords_to_remove(list, optional): List of words to remove from stopwords (word.is_stop=False)\n",
    "        spacy_model(string, optional): String to select a spacy language model. (Defaults to \"en_core_web_sm\".)\n",
    "                            Additional Options:  \"en_core_web_md\", \"en_core_web_lg\"; \n",
    "                            (Must first download the model by name in the terminal:\n",
    "                            e.g.  \"python -m spacy download en_core_web_lg\" )\n",
    "            \n",
    "    Returns:\n",
    "        nlp pipeline: spacy pipeline with special cases and updated nlp.Default.stopwords\n",
    "    \"\"\"\n",
    "    # Load the English NLP model\n",
    "    nlp = spacy.load(spacy_model, disable=disable)\n",
    "    \n",
    "    # Adding Special Cases \n",
    "    # Loop through the contractions list and add special cases\n",
    "    for contraction in contractions:\n",
    "        special_case = [{\"ORTH\": contraction}]\n",
    "        nlp.tokenizer.add_special_case(contraction, special_case)\n",
    "    \n",
    "    # Adding stopwords\n",
    "    for word in stopwords_to_add:\n",
    "        # Set the is_stop attribute for the word in the vocab dict to true.\n",
    "        nlp.vocab[\n",
    "            word\n",
    "        ].is_stop = True  # this determines spacy's treatmean of the word as a stop word\n",
    "        # Add the word to the list of stopwords (for easily tracking stopwords)\n",
    "        nlp.Defaults.stop_words.add(word)\n",
    "    \n",
    "    # Removing Stopwords\n",
    "    for word in stopwords_to_remove:\n",
    "        \n",
    "        # Ensure the words are not recognized as stopwords\n",
    "        nlp.vocab[word].is_stop = False\n",
    "        nlp.Defaults.stop_words.discard(word)\n",
    "        \n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675735c-d4a0-458b-a7bf-10843606e018",
   "metadata": {},
   "source": [
    "**Test the function**\n",
    "\n",
    "We will now define a custom pipeline and include and include it in our preprocessing function. Since we are processing a single string with our batch preprocessing function, we will need to pass it into the function as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ccc54c9-2885-44dd-9c6c-97e503b28269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:21, 21.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['while', 'running', 'central', 'noticed', 'constant', 'buzzing', 'flies', 'annoying', \"don't\", 'like', 'flies', 'but', 'attracted', 'mcdonald', 'someone', 'dropped', 'wondered', 'uncaring']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Customize the nlp pipeline\n",
    "function_nlp = make_custom_nlp(    \n",
    "    disable=['ner', 'parser'],\n",
    "    contractions=[\"don't\"],\n",
    "    stopwords_to_add=['park'],\n",
    "    stopwords_to_remove=['while'],\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    ")\n",
    "# call preprocessing function with custom nlp pipeline\n",
    "tokens = batch_preprocess_texts([sample_text], nlp = function_nlp)\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e752ba-91e9-4efd-a4df-01c034f7b0be",
   "metadata": {},
   "source": [
    "Check the results to confirm that the outcome was what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f86fb-b58c-418c-b4cf-a23ef9a9ad56",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3deca-4f70-45a0-b78d-c8bef5d6a95b",
   "metadata": {},
   "source": [
    "In this lesson, you learned how to customize your nlp preprocessing pipeline by adding custom stop words specific to a text, how to remove stop words from the standard 'english' stop word list, and how to tell SpaCy to keep contractions together.  You now have a  function to create a customized nlp object to use in the preprocess_text function you learned earlier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
